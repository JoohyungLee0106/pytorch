{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WRITING CUSTOM DATASETS, DATALOADERS AND TRANSFORMS\n",
    "## 1. Goal: To develop a dataloader that provides 1) stacked images, 2) paired segmentation mask, 3) classification label\n",
    "```\n",
    "Author: Joohyung Lee\n",
    "References: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "            https://jdhao.github.io/2017/10/23/pytorch-load-data-and-make-batch/\n",
    "            https://pytorch.org/docs/stable/data.html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture in-summary\n",
    "```\n",
    "Two classes are needed\n",
    "```\n",
    "* **Class 1:** dataset (torch.utils.data.Dataset or torch.utils.data.IterableDataset)\n",
    " * Enables access of each sample by its index\n",
    " * It can output a tuple, list, or dictionary of required data- e.g., {'image': ..., 'mask': ..., 'category': ...}  \n",
    " * Built-in class: torchvision.datasets.ImageFolder\n",
    " * Augmentation by cascading a series of transforms by providing a list of transforms to torchvision.transforms.Compose\n",
    " \n",
    "&nbsp;\n",
    "* **Class 2:** torch.utils.data.DataLoader\n",
    " * Creates a data batch\n",
    " * Iterator that receives torch.utils.data.Dataset object with various useful functionalities-e.g., batching, shuffling, multi-processing\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset\n",
    "### 3-1. torch.utils.data.Dataset\n",
    "```\n",
    "Inherit built-in Dataset and override these methods:\n",
    "```\n",
    "* **`__init__:`** Constructor\n",
    " * Read needed files (e.g., *.csv, *.txt, etc) but do NOT actually read the image\n",
    "<br/><br/>\n",
    "\n",
    "* **`__len__:`** len(custom_dataset) will return its output\n",
    "<br/><br/>\n",
    "\n",
    "* **`__getitem__:`** custom_dataset[i] will return its output (ith sample)\n",
    " * Read called images\n",
    "<br/><br/>\n",
    "\n",
    "* Example for Overriding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Inherit the built-in Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **torchvision.transforms.Compose:** write callable classes for various augmentations, image-size equalization within a mini-batch\n",
    " * Class instead of function so that parameters need not be fed everytime it's called\n",
    " * transforms (list of Transform objects) â€“ list of transforms to compose (from left element to right element in list)\n",
    " * Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, utils\n",
    "\n",
    "class Rescale(object):\n",
    "    def __init__(self, output_size):\n",
    "        ...\n",
    "    def __call__(self, sample):\n",
    "        return ...\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, output_size):\n",
    "        ...\n",
    "    def __call__(self, sample):\n",
    "        return ...\n",
    "\n",
    "# Rescale followed by randomcrop (left to right)\n",
    "composed = transforms.Compose([Rescale(256), RandomCrop(224)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1-1. torchvision.datasets.ImageFolder\n",
    "* Retreives images assuming the following heirarchy:\n",
    "    * root/category/xxx.png\n",
    "    * extension can be heterogeneous (png, jpg, jpeg, etc)\n",
    "    * Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. torch.utils.data.IterableDataset\n",
    "```\n",
    "Represents an iterable over data samples and override these methods:\n",
    "```\n",
    "* **`__init__:`** Constructor\n",
    "<br/><br/>\n",
    "\n",
    "* **`__iter__:`**\n",
    " * Returns an iterator of samples in dataset\n",
    "<br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. torch.utils.data.DataLoader\n",
    "* Iterator with the following functionalities:\n",
    "    * Batching\n",
    "    * Shuffling\n",
    "    * Collate\n",
    "    * `multiprocessing` to load the data in parallel\n",
    "        *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None,\n",
    "                            num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0,\n",
    "                            worker_init_fn=None, multiprocessing_context=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.utils.data.sampler.WeightedRandomSampler\n",
    "* SequentialSampler\n",
    "* RandomSampler\n",
    "* SubsetRandomSampler\n",
    "* WeightedRandomSampler\n",
    "* BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collate_fn\n",
    "* To pack a series of images and labels as tensors (first dimension is batch-size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "       data: is a list of tuples with (example, label, length)\n",
    "             where 'example' is a tensor of arbitrary shape\n",
    "             and label/length are scalars\n",
    "    \"\"\"\n",
    "    _, labels, lengths = zip(*data)\n",
    "    max_len = max(lengths)\n",
    "    n_ftrs = data[0][0].size(1)\n",
    "    features = torch.zeros((len(data), max_len, n_ftrs))\n",
    "    labels = torch.tensor(labels)\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        j, k = data[i][0].size(0), data[i][0].size(1)\n",
    "        features[i] = torch.cat([data[i][0], torch.zeros((max_len - j, k))])\n",
    "\n",
    "    return features.float(), labels.long(), lengths.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.RandomSizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "#     for 3 (RGB) channels\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "hymenoptera_dataset = datasets.ImageFolder(root='hymenoptera_data/train',\n",
    "                                           transform=data_transform)\n",
    "dataset_loader = torch.utils.data.DataLoader(hymenoptera_dataset,\n",
    "                                             batch_size=4, shuffle=True,\n",
    "                                             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "* https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "* https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
